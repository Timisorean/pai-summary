\section{Fundamentals}

\begin{framed}
    \begin{itemize}
        \item \textbf{Normal}:
            % $\begin{aligned}[t]
            %     &(\sqrt{2\pi \sigma^2})^{-1}\exp(-\frac{(x-\mu)^2}{2\sigma^2}) \\[-1ex]
            %     &\frac{\exp (-\frac{1}{2}(\vx - \vmu)^T \Sigma^{-1} (\vx - \vmu) )}{\sqrt{(2\pi)^k \det{\Sigma}}}
            % \end{aligned}$
             $\frac{\exp (-\frac{1}{2}(\vx - \vmu)^T \Sigma^{-1} (\vx - \vmu) )}{\sqrt{(2\pi)^k \det{\Sigma}}}$
        \item \textbf{Beta}: $\Beta[\theta]{\alpha}{\beta} \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}$ 
        \item \textbf{Laplace}: $\frac{1}{2l}\exp(-\frac{\abs{x - \mu}}{l})$
    \end{itemize}
\end{framed}

\begin{itemize}
    \item Gaussian CDF has no closed-form; $O(n^2)$ params.
\end{itemize}

\begin{framed}
    \begin{itemize}
        \item $\E{\mA \rX + \vb} = \mA \E{\rX} + \vb$;  $\E{\rX + \rY} = \E{\rX} + \E{\rY}$
        \item $\E{\rX\transpose{\rY}} = \E{\rX} \cdot \transpose{\E{\rY}}$ (if $\rX$, $\rY$ indep.)
        \item LOTUS: $\E{\vg(\rX)} = \int_{\rX(\Omega)} \vg(\vx) \cdot p(\vx) \,d\vx$ \\[-1ex]
        (if $\vg$ nice and $\rX$ cont.)
        \item Tower rule: $\E[\rY]{\E[\rX]{\rX \mid \rY}} = \E{\rX}$
    \end{itemize}
    \vspace{0.2mm}\hrule\vspace{0.2mm}
    \begin{itemize}
        \item
            $\begin{aligned}[t]
                \Var{\rX} &\defeq \E{(\rX - \E{\rX})\transpose{(\rX - \E{\rX})}} \\[-1ex]
                &= \E{\rX \transpose{\rX}} - \E{\rX} \cdot \transpose{\E{\rX}} = \Cov{\rX, \rX}
            \end{aligned}$
        \item $\Var{\mA\rX + \vb} = \mA\Var{\rX}\transpose{\mA}$
        \item $\Var{\rX + \rY} = \Var{\rX} + \Var{\rY} + 2 \Cov{\rX, \rY}$
        \item $\Var{\rX + \rY} = \Var{\rX} + \Var{\rY}$ (if $\rX$, $\rY$ indep.)
        \item Law of total variance, LOTV: \\
        $\Var{\rX} = \E[\rY]{\Var[\rX]{\rX \mid \rY}} + \Var[\rY]{\E[\rX]{\rX \mid \rY}}$
    \end{itemize}
    \vspace{0.2mm}\hrule\vspace{0.2mm}
    \begin{itemize}
        \item
            $\begin{aligned}[t]
                \Cov{\rX, \rY} &\defeq \E{(\rX - \E{\rX})\transpose{(\rY - \E{\rY})}} \\[-1ex]
                &= \E{\rX \transpose{\rY}} - \E{\rX} \cdot \transpose{\E{\rY}}
            \end{aligned}$
        \item $\Cov{\rX, \rY} = \Cov{\rY, \rX}$; $\Cov{\rX, \rY} \ge \mzero$
        \item $\Cov{\mA\rX + \vc, \mB\rY + \vd} = \mA\Cov{\rX,\rY}\transpose{\mB}$
    \end{itemize}
\end{framed}

\begin{itemize}
    \item \emph{Correlation} is normalized covariance:\\
    $\Cor{\rX,\rY}(i,j) \defeq \frac{\Cov{X_i,Y_j}}{\sqrt{\Var{X_i} \Var{Y_j}}} \in [-1,1]$
    \item \emph{Uncorrelated} iff $\Cov{\rX, \rY} = \mzero$.
    \item \textbf{Change of variables:} Let $\vg$ be diff. and inv. Then for $\rY = \vg(\rX)$: $p_\rY(\vy) = p_\rX(\inv{\vg}(\vy)) \cdot \abs{\det{\jac \inv{\vg}(\vy)}}$ where $\jac \inv{\vg}(\vy)$ is the Jacobian of $\inv{\vg}$ at $\vy$.
\end{itemize}

% \begin{framed}
%     \textbf{Posterior} $p(\vx \mid \vy)$: updated belief about $\vx$ after observing $\vy$. \textbf{Prior} $p(\vx)$: initial belief about $\vx$.
%     \textbf{Conditional likelihood} $p(\vy \mid \vx)$: how likely observations $\vy$ are under given value $\vx$.
%     \textbf{Joint likelihood} $p(\vx, \vy) = p(\vy \mid \vx) p(\vx)$
%     \textbf{Marginal likelihood} $p(\vy)$: how likely observations $\vy$ are across all values of $\vx$: $p(\vy) = \int_{\rX(\Omega)} p(\vy \mid \vx) \cdot p(\vx) \,d\vx$. 
% \end{framed}

\begin{framed}
    \textbf{Bayes' rule}: $p(\vx \mid \vy) = \frac{p(\vy \mid \vx) \cdot p(\vx)}{p(\vy)}$
\end{framed}

\begin{itemize}
    \item If prior $p(\vx)$ and posterior $p(\vx \mid \vy)$ from same fam. of distr., prior is \textbf{conjugate prior} to likelihood $p(\vy \mid \vx)$.
    \item Beta distr. is a conjugate prior to binomial likelihood.
    \item Under some conditions, \textbf{Gaussian is self-conjugate} (Gaussian prior and likelihood $\rightarrow$ posterior Gaussian).
\end{itemize}

\vspace{0.2mm}\hrule\vspace{0.2mm}

\begin{itemize}
    \item Choosing non-informative prior in absence of evidence is \textbf{principle of indifference/insufficient reason}.
    \item \textbf{Improper prior:} not required that prior is a valid distr. (i.e., integrates to 1). Can still derive meaning.
    \item \textbf{Max. entropy principle:} choose prior s.t. one that makes the least “additional assumptions”, i.e., prior least “informative”.
\end{itemize}

\vspace{0.2mm}\hrule\vspace{0.2mm}

\textbf{Gaussian properties}
\begin{itemize}
    \item \textbf{Gaussians have max. entropy among all distr.}\\[-0.5ex]
    with known mean and var.: $\sfrac{1}{2} \cdot \log \parentheses{(2 \pi e)^d \det{\mSigma}}$
    % \item \scalebox{0.9}{$\I{\rX}{\rY} = \frac12 \log \det{\mI + \sigma_n^{-2} \mSigma)}$}
    \item Jointly Gaussian random vectors, $\rX$ and $\rY$, are independent iff $\rX$ and $\rY$ are uncorrelated.
    \item Closed under marginalization and conditioning.
    \begin{framed}
        Let $\rX$ be Gaussian and index sets $\sA, \sB \subseteq [n]$. \\
        For any \textbf{marginal distr.} $\rX_A \sim \N{\vmu_\sA}{\mSigma_{\sA\sA}}$ and \\
        for any \textbf{conditional distr.}: \\
        $\begin{aligned}[t]
            \rX_\sA \mid \rX_\sB &= \vx_\sB \sim \N{\vmu_{\sA \mid \sB}}{\mSigma_{\sA \mid \sB}} \text{ where} \\[-1.5ex]
            \vmu_{\sA \mid \sB} &\defeq \vmu_\sA + \mSigma_{\sA\sB}\inv{\mSigma_{\sB\sB}}(\vx_\sB - \vmu_\sB) \\[-1.5ex]
            \mSigma_{\sA \mid \sB} &\defeq \mSigma_{\sA\sA} - \mSigma_{\sA\sB}\inv{\mSigma_{\sB\sB}}\mSigma_{\sB\sA}
        \end{aligned}$ \\
        Observe that the variance can only shrink.
    \end{framed}
    \item Additive and closed under affine transformations.
    \begin{framed}
        \begin{itemize}
            \item \scalebox{0.9}{$M \cdot \N{\vmu}{\mSigma} = \N{M \mu}{\transpose M \mSigma M}$}
            \item \scalebox{0.9}{$\N{\vmu_A}{\mSigma_A} + \N{\vmu_B}{\mSigma_B} = \N{\vmu_A + \vmu_B}{\mSigma_A + \mSigma_B}$}
            \item \scalebox{0.9}{$\N{\vmu_A}{\mSigma_A} \cdot \N{\vmu_B}{\mSigma_B} \propto \N{\cdot}{\cdot}$}
        \end{itemize}
    \end{framed}
\end{itemize}

\vspace{0.2mm}\hrule\vspace{0.2mm}

\textbf{Maximum likelihood estimate (MLE)}: \\[-0.5ex]
\scalebox{0.9}{$\vthetahat_\MLE \defeq \underset{\vtheta \in \Theta}{\argmax} p(y_{1:n} \mid \vx_{1:n}, \vtheta) = \underset{\vtheta \in \Theta}{\argmax} \sum_{i=1}^n \log p(y_i \mid \vx_i, \vtheta)$}\vspace{-3pt}
\begin{itemize}
    \item \textbf{Consistent} if: $\vthetahat_\MLE \convp \opt{\vtheta}$ as $n \to \infty$.
    \item \textbf{Asymptotically normal} if $\vthetahat_\MLE \convd \N{\opt{\vtheta}}{\mS_n}$ as $n \to \infty$ where $\mS_n$ is asymptotic covar. of MLE.
    \item MLE is \textbf{asymptotically efficient} (there exists no other consistent estimator with a “smaller” asymptotic var.).
    \item For the finite sample regime, the MLE need not be unbiased, and it is susceptible to overfitting to the (finite) training data.
\end{itemize}

\vspace{0.2mm}\hrule\vspace{0.2mm}

\textbf{Maximum a posterior (MAP) estimate}:\\[-0.5ex]
$\vthetahat_\MAP \defeq \argmin_{\vtheta \in \Theta} \underbrace{- \log p(\vtheta)}_{\text{regularization}} + \underbrace{\ell_\mathrm{nll}(\vtheta; \spD_n)}_{\text{quality of fit}}$\\
The \textbf{log-prior} $\log p(\vtheta)$ acts as a regularizer. Common:
\begin{itemize}
    \item $p(\vtheta) = \N[\vtheta]{\vzero}{\lambda \mI}$ gives $-\log p(\vtheta) = \frac{\lambda}{2} \norm{\vtheta}_2^2 + \const$
    \item $p(\vtheta) = \Laplace[\vtheta]{\vzero}{\lambda}$ gives $-\log p(\vtheta) = \lambda \norm{\vtheta}_1 + \const$
    \item Uniform prior gives $\const$ (no reg., MAP is MLE)
\end{itemize}
