\vspace{0.5mm}\hrule\vspace{0.5mm}
\section{Bayesian Optimization}

\begin{framed}
    \textbf{Cumulative regret} for time horizon $T$ associated with choices $\{\vx_t\}_{t=1}^T$ is: $R_T \defeq \sum_{t=1}^T \underbrace{({\max_\vx \opt{f}(\vx) - \opt{f}(\vx_t)})}_{\text{\textit{instantaneous regret}}}$.
\end{framed}

Goal: Achieve sublinear regret: $\lim_{T\to\infty} \sfrac{R_T}{T} = 0$ (requires balancing exploration and exploitation).

\vspace*{-1.5pt}
\includegraphics[width=0.8\linewidth]{images/Bayesian_Optimization.png}

\begin{itemize}
    \item Common to use an \textbf{acquisition fn.} to greedily pick the next point to sample based on the current model.
    \item \textbf{Upper confidence bound (UCB)}: $\vx_{t+1} \defeq \argmax_{\vx \in \spX} \mu_{t}(\vx) + \beta_{t+1} \sigma_{t}(\vx)$, where $\sigma_t(\vx) \defeq \sqrt{k_t(\vx, \vx)}$. If $\beta_t = 0$ then UCB is purely exploitative; if $\beta_t \to \infty$, UCB recovers uc sampling. UCB fn. generally non-convex.
\end{itemize}

\begin{framed}
    When choosing $\beta_t$ appropriately: \scalebox{0.9}{$R_T = \BigO{\sqrt{T \gamma_T}}$}, with \scalebox{0.9}{$\gamma_T \defeq \max_{\substack{\sS \subseteq \spX \\ \card{\sS} = T}} \I{\vf_\sS}{\vy_\sS} = \max_{\substack{\sS \subseteq \spX \\ |\sS| = T}} \frac{1}{2} \log\det{\mI + \sigman^{-2} \mK_{\sS\sS}}$}, is the maximum information gain after $T$ rounds.
    \begin{itemize}
        \item Linear: $\gamma_T = \BigO{d \log T}$\vspace{-0.5ex} 
        \item Gaussian: $\gamma_T = \BigO{(\log T)^{d+1}}$\vspace{-1ex}
        \item MatÃ©rn for $\nu > \frac{1}{2}$: $\gamma_T = \BigO{T^{\frac{d}{2\nu + d}} (\log T)^{\frac{2\nu}{2\nu + d}}}$
    \end{itemize}
\end{framed}

\textbf{Thompson Sampling}: At time $t+1$, we sample a fn. $\Tilde{f}_{t+1} \sim p(\cdot \mid \vx_{1:t}, y_{1:t})$ from our posterior distr.
Then, we simply max. $\Tilde{f}_{t+1}$, $\vx_{t+1} \defeq \argmax_{\vx \in \spX} \Tilde{f}_{t+1}(\vx)$.
