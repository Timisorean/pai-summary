\vspace{0.2mm}\hrule\vspace{0.2mm}
\section{Diffusion generative models}

% \begin{framed}
%     \textbf{MCs}: A seq. $(X_t)_{t \in \Nat_0} \in \sS$ with \scalebox{0.9}{$\sS \defeq \{0, \dots, n-1\}$}, s.t. the \textbf{Markov property}: \scalebox{0.9}{$X_{t+1} \perp X_{0:t-1} \mid X_t$} is satisfied. 
% \end{framed}

\vspace{-2pt}
Let $\beta_t \in (0,1]$, $\bar\alpha_t = \prod_{s=1}^t \alpha_s$, and $\alpha_s = 1 - \beta_s$. Typically, $\beta_t$ is monotonically increases, which implies that $\bar\alpha_t \rightarrow 0$ and thus $x_T \rightarrow \N{0}{\mI}$ for $T \rightarrow \infty$. 

\begin{enumerate}
    \item \textbf{Forward process:} Transform data points into (Gaussian) noise by using a fixed noising MC $q$:\\[-0.5ex]
    $q(x_{1:T} \mid x_0) = \prod_{t=1}^T q(x_t \mid x_{t-1})$ (MC-chain)\\
    $q(x_{t} \mid x_{t-1}) = \N[x_t]{\sqrt{1 - \beta_t}x_{t-1}}{\beta_t \mI}$\\
    $q(x_{t} \mid x_0) = \N[x_t]{\sqrt{\bar\alpha_t}x_0}{(1-\bar\alpha_t) \mI}$


    \item \textbf{Backward process:} Learn a denoising MC $p$ matching the reversed forward process.\\
    $p_\lambda(x_{t-1} \mid x_t) = \N[x_{t-1}]{\mu_\lambda(x_t, t)}{\mSigma_\lambda(x_t, t)}$\\
    $p_\lambda(x_{0:T}) = p_\lambda(x_T) \prod_{t=1}^T p_\lambda(x_{t-1} \mid x_t)$ \\
    $p_\lambda(x_0) = \int p_\lambda(x_{0:T}) \,d x_{1:T}$ where $x_{1:T}$ latent vars.
    
    
    \item \textbf{Generation:} Now generate novel data points by simulating the learned denoising MC $p$. Sample seq.:\\
    (1) \scalebox{0.9}{$x_1 \sim p(X_1)$}, (2) \scalebox{0.9}{$x_2 \sim p(X_2 \mid X_1 = x_1)$}, ...
\end{enumerate}

\columnbreak
\vspace*{-12pt}

Use VI. ELBO to get intractable $p_\lambda(x_0)$:

\scalebox{0.87}{$\begin{aligned}[t]
&\log p_\lambda(x_0) \geq \log p_\lambda(x_0) - \spa{D}_{\mathrm{KL}}(q(\cdot \mid x_0) \| p_\lambda(\cdot \mid x_0))\\[-1.3ex]
% &= \E[x_{1:T} \sim q(\cdot \mid x_0)]{\log p_\lambda(x_0) - \log \frac{q(x_{1:T} \mid x_0)}{p_\lambda(x_{1:T} \mid x_0)}} \\[-1ex]
% &= \E[x_{1:T} \sim q(\cdot \mid x_0)]{\log p_\lambda(x_0) - \log \frac{q(x_{1:T} \mid x_0) p_\lambda(x_0)}{p_\lambda(x_{0:T})}} \\[-1ex]
% &= \E[x_{1:T} \sim q(\cdot \mid x_0)]{\log p_\lambda(x_T) - \sum_{t=1}^T \log \frac{q(x_t \mid x_{t-1})}{p_\lambda(x_{t-1} \mid x_t)}} \\[-1ex]
% &= \E["]{\log p_\lambda(x_T) - \sum_{t=2}^T \log \frac{q(x_t \mid x_{t-1})}{p_\lambda(x_{t-1} \mid x_t)} - \log \frac{q(x_1 \mid x_0)}{p_\lambda(x_0 \mid x_1)}}\\[-3.3ex]
% &= \E["]{\log p_\lambda(x_T) - \sum_{t=2}^T \log \parentheses{\frac{q(x_{t-1} \mid x_t, x_0)}{p_\lambda(x_{t-1} \mid x_t)} \cdot \frac{q(x_t \mid x_0)}{q(x_{t-1} \mid x_0)}} - \log \frac{q(x_1 \mid x_0)}{p_\lambda(x_0 \mid x_1)}} \\[-2.5ex]
% &= \E["]{\log \frac{p_\lambda(x_T)}{q(x_T \mid x_0)} - \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_\lambda(x_{t-1} \mid x_t)} + \log p_\lambda(x_0 \mid x_1)} \\[-2ex]
&= \const + \E["]{- \sum_{t=2}^T \underbrace{\spa{D}_{\mathrm{KL}}(q(\cdot \mid x_t, x_0) \| p_\lambda(\cdot \mid x_t))}_{L_t} + \underbrace{\log p_\lambda(x_0 \mid x_1)}_{L_1}}
\end{aligned}$}\\[-1ex]
w. \scalebox{0.73}{$" \defeq x_{1:T} \sim q(\cdot \mid x_0)$}. Assume const. var. schedule $\Sigma_t = \sigma_t^2 \mI$ and optimize \textbf{closed-forms} via \textbf{stochastic VI}:\\
\scalebox{0.95}{$L_t = \sfrac{1}{2\sigma_t^2} \norm{\mu_t'(x_t, x_0) - \mu_\lambda(x_t, t)}_2^2 + \const$}\\
% \scalebox{0.95}{$L_1 = \sfrac{1}{2\sigma_1^2} \norm{\mu_1'(x_1, x_0) - \mu_\lambda(x_1, 1)}_2^2 + \const$}\\
Closed-form $\mu_t'(x_t, x_0)$ (can be highly non-stationary):\\
\scalebox{0.9}{$\mu_t'(x_t, x_0) = \frac{\sqrt{\mean{\alpha}_{t}} \beta_t}{1 - \mean{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1 - \mean{\alpha}_{t-1})}{(1 - \mean{\alpha}_t)} x_t$}, use rep. trick: $x_t(x_0, \epsilon) = \sqrt{\bar\alpha_t} x_0 + \sqrt{1 - \bar\alpha_t} \epsilon$ and learn sample noise.\\
Sample: $x_{t-1} = \frac{1}{\sqrt{\alpha_{t}}}(x_{t} - \frac{1-\alpha_{t}}{\sqrt{1-\overline{\alpha_{t}}}}\epsilon_{\lambda}(x_{t},t)) + \sigma_{t}z$