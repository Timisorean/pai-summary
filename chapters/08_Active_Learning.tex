\vspace{0.2mm}\hrule\vspace{0.2mm}
\section{Active Learning}

\begin{framed}
    \textbf{Cond. entropy}: $\begin{aligned}[t]
        &\H{\rX \mid \rY} \defeq \E[\vy \sim p(\vy)]{\H{\rX \mid \rY = \vy}} \\[-1ex]
        &= \E[(\vx, \vy) \sim p(\vx, \vy)]{- \log p(\vx \mid \vy)}
    \end{aligned}$
    \vspace{0.2mm}\hrule\vspace{0.2mm}
    \textbf{Joint entropy}: $\H{\rX, \rY} \defeq \E[(\vx, \vy) \sim p(\vx, \vy)]{- \log p(\vx, \vy)}$ 
    \begin{itemize}
        \item $\text H[\rX \mid \rY] \ne \text H[\rY \mid \rX]$ in general; but $\text H[\rX, \rY] = \text H[\rY, \rX]$
        \item $\H{\rX, \rY} = \H{\rY} + \H{\rX \mid \rY} = \H{\rX} + \H{\rY \mid \rX}$ 
        \item $\H{\rX \mid \rY} = \H{\rY \mid \rX} + \H{\rX} - \H{\rY}$ (Bayes Rule)
        \item $\H{\rX \mid \rY} \leq \H{\rX}$ (Gibbs; Information never hurts)\\
        $\Leftrightarrow 0 \le \H{\rX} - \H{\rX \mid \rY} = \I{\rX}{\rY}$
    \end{itemize}
    \vspace{0.2mm}\hrule\vspace{0.2mm}
    \textbf{Mutual info}: $\I{\rX}{\rY} \defeq \H{\rX} + \H{\rY} - \H{\rX, \rY}$
    \begin{itemize}
        \item $\I{\rX}{\rY} = \I{\rY}{\rX} = \E[\vy \sim p]{\KL{p(\vx \mid \vy)}{p(\vx)}}$
    \end{itemize}
    \textbf{Cond. mutual info}:
    \begin{itemize}
        \item $\begin{aligned}[t]
            \I{\rX}{\rY}[\rZ] &= \H{\rX \mid \rZ} - \H{\rX \mid \rY, \rZ} \\[-1ex]
            &= \H{\rX, \rZ} + \H{\rY, \rZ} - \H{\rZ} - \H{\rX, \rY, \rZ} \\[-1ex]
            &= \I{\rX}{\rY, \rZ} - \I{\rX}{\rZ}
        \end{aligned}$
        \item $\I{\rX}{\rY \mid \rZ} = \I{\rY}{\rX \mid \rZ}$
        \item $\I{\rX}{\rY; \rZ} \defeq \I{\rX}{\rY} - \I{\rX}{\rY \mid \rZ}$, so the “information never hurts” principle does not hold for MI. Information about $Z$ may reduce the MI between $\rX$ and $\rY$ 
    \end{itemize}
\end{framed}

\begin{framed}
    \begin{itemize}
        \item Given (discrete) fn. $F : \pset{\spX} \to \R$, the \textbf{marginal gain} of $\vx \in \spX$ given $\sA \subseteq \spX$ is: \scalebox{0.8}{$\Delta_F(\vx \mid \sA) \defeq F(\sA \cup \{\vx\}) - F(\sA)$}.
        \item The fn. is \textbf{submodular} iff for any $\vx \in \spX$ and any $\sA \subseteq \sB \subseteq \spX$: \scalebox{0.9}{$F(\sA \cup \{\vx\}) - F(\sA) \geq F(\sB \cup \{\vx\}) - F(\sB)$} or equally \scalebox{0.9}{$\Delta_F(\vx \mid \sA) \ge \Delta_F(\vx \mid \sB)$}. Submodularity can be interpreted as notion of “concavity” for discrete fns.
        \item It is called \textbf{monotone} if $F(\sA) \leq F(\sB)$.
    \end{itemize}
\end{framed}

\begin{framed}
    \textbf{Maximization objective}: monotone submodular function: $I(\sS) \defeq \I{\vf_\sS}{\vy_\sS} = \H{\vf_\sS} - \H{\vf_\sS \mid \vy_\sS}$. $\H{\vf_\sS}$: uc in $\vf_\sS$ before observing $\vy_\sS$. $\H{\vf_\sS \mid \vy_\sS}$ uc in $\vf_\sS$ after observing $\vy_\sS$. Max. MI is in general NP-hard.
\end{framed}

\begin{itemize}
    \item \textbf{Greedy}: Pick $\vx_1$ through $\vx_n$ individually by greedily finding the location with the maximal MI, this provides a $(1 - \nicefrac{1}{e})$-approximation of the optimum. 
    \item \textbf{Uncertainty sampling}: Have already picked $\sS_t = \{\vx_1, \dots, \vx_t\}$; Solve the following: \scalebox{0.9}{$\vx_{t+1} \defeq \argmax_{\vx \in \spX} \Delta_I(\vx \mid \sS_t) = \argmax_{\vx \in \spX} \Ism{f_\vx}{y_{\vx} \mid \vy_{\sS_t}}$}. Doesn't work with heteroscedastic noise: large aleatoric uc may dominate epistemic uc. In classification corresponds to selecting label that max. entropy of predicted label: $\vx_{t+1} \defeq \argmax_{\vx \in \spX} \H{y_{\vx} \mid \vx_{1:t}, y_{1:t}}$. 
\end{itemize}

\begin{framed}
    \scalebox{0.9}{\textbf{Bayesian active learning by disagreement (BALD)}:} Identifies points $\vx$ where models \emph{disagree} about label $y_{\vx}$ (each model is \textit{confident} but predict different labels):\\
    $\vx_{t+1} \defeq \argmax_{\vx \in \spX} \I{\vtheta}{y_{\vx} \mid \vx_{1:t}, y_{1:t}} = \argmax_{\vx \in \spX} \H{y_{\vx} \mid \vx_{1:t}, y_{1:t}} - \E*[\vtheta \mid \vx_{1:t}, y_{1:t}]{\H{y_{\vx} \mid \vtheta}}$
\end{framed}

\begin{itemize}
    \item \textbf{Inductive learning} extract general rules from data. Typically, we can directly observe $f(\vx)$ at any $\vx$.
    \item \textbf{Transductive learning} make best pred. at particular $\vxs$. Typically, cannot directly observe $f(\vxs)$. Require gen. $f(\vs)$ from the behavior of $f$ at other locations.
\end{itemize}