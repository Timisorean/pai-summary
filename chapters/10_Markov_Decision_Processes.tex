\vspace{0.5mm}\hrule\vspace{0.5mm}
\section{Markov Decision Processes (MDPs)}

\begin{framed}
    A \textbf{(finite) MDP} is spec. by a (finite) \textbf{states} set $\sX \defeq \{1, \dots, n\}$; a (finite) \textbf{actions} set $\sA \defeq \{1, \dots, m\}$; \textbf{transition probs.} ${p(x' \mid x, a) \defeq \Pr{X_{t+1} = x' \mid X_t = x, A_t = a}}$; a \textbf{reward fn.} $r : X \times A \to \R$.
\end{framed}

% $r$ induces a sequence of rewards: $R_t \defeq r(X_t, A_t)$.

\begin{framed}
    A \textbf{policy} maps each state $x \in \sX$ to a prob. distr. over actions. For any $t > 0$: $\pi(a \mid x) \defeq \Pr{A_t = a \mid X_t = x}$.
\end{framed}

\begin{itemize}
    \item A policy induces a MC $(X_t^\pi)_{t\in\Nat_0}$: $p^\pi(x' \mid x) \defeq \Pr{X_{t+1}^\pi = x' \mid X_t^\pi = x} = \sum_{a \in \sA} \pi(a \mid x) p(x' \mid x, a)$.
    \item The \textbf{discounted payoff} from time $t$ is: $G_t \defeq \sum_{m=0}^\infty \gamma^m R_{t+m}$, for \textbf{discount factor} $\gamma \in [0, 1)$.
\end{itemize}

\begin{framed}
    \begin{itemize}
        \item \textbf{State value fn.}: \scalebox{0.8}{$v_t^\pi \defeq \E[\pi]{G_t \mid X_t = x, A_t = a}$} measures avg. discounted payoff from time $t$ starting from $x \in \sX$.
        \item \textbf{State-action value fn. (Q-fn.)}: \scalebox{0.8}{$\q[\pi]{x}{a}[t] \defeq$} \scalebox{0.8}{$\E[\pi]{G_t \mid X_t = x, A_t = a} = r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \cdot \v[\pi]{x'}[t+1]$} measures avg. discounted payoff from time $t$ starting from $x \in \sX$ and with playing action $a \in \sA$.
    \end{itemize}
\end{framed}

\begin{framed}
    \textbf{Bellman Expectation Equation}: 
    \begin{itemize}
        \item $\v[\pi]{x} = r(x, \pi(x)) + \gamma \E[x' \mid x, \pi(x)]{\v[\pi]{x'}}$
        \item If stochastic policy: $\v[\pi]{x} = \E[a \sim \pi(x)]{\q[\pi]{x}{a}}$\\
        $\q[\pi]{x}{a} = r(x, a) + \gamma \E*[x' \mid x, a]{\E[a' \sim \pi(x')]{\q[\pi]{x'}{a'}}}$
        \item For deterministic: $\v[\pi]{x} = \q[\pi]{x}{\pi(x)}$.
    \end{itemize}
\end{framed}

Can be used to find $\fnv[\pi]$ given policy $\pi$, by solving linear system of eq. in cubic time in size of state space. Can also be solved using fixed pt. iter: $\mB^\pi \vv \defeq \vr^\pi + \gamma \mP^\pi \vv$.

\scalebox{0.7}{$\norm{\vv_t^\pi - \vv^\pi}_\infty = \norm{\mB^\pi \vv_{t-1}^\pi - \mB^\pi \vv^\pi}_\infty \le \gamma \norm{\vv_{t-1}^\pi - \vv^\pi}_\infty \le \gamma \norm{\vv_0^\pi - \vv^\pi}_\infty$}

\begin{framed}
    \begin{itemize}
        \item A \textbf{greedy policy} w.r.t. to a state-action value fn. $\fnq$ is $pi_{\fnq}(x) \defeq \argmax_{a \in \sA} \q{x}{a}$;
        \item a \textbf{greedy policy} w.r.t. a state value fn. $\fnv$ is: $\pi_{\fnv}(x) \defeq \argmax_{a \in \sA} r(x, a) + \gamma \sum_{x' \in \sX} p(x' \mid x, a) \cdot \v{x'}$.
    \end{itemize}
\end{framed}

\begin{framed}
    \textbf{Bellman's theorem}: A policy $\pis$ is optimal iff it's greedy w.r.t its own value fn. In other words, $\pis$ is optimal iff $\pis(x)$ is a distr. over set $\argmax_{a \in \sA} \q*{x}{a}$.
\end{framed}

\noindent
\begin{tabular}{@{}p{0.55\columnwidth}@{\extracolsep{\fill}}p{0.45\columnwidth}@{}}
    \color{black}
    \vspace{-5pt}
    \begin{itemize}
        \item If for every state there is a unique action that max. the q-fn., $\pi^\star$ is deter. and unique, \scalebox{0.9}{$\pi^\star(x) = \text{argmax}_{a \in A} q^\star(x, a)$}.
        \item  For finite MDPs, PI converges to an optimal policy in a poly. num. of iter. Each step takes cubic complexity in the num. of states.
    \end{itemize}& 
    
    \adjustbox{valign=t}{\includegraphics[width=\linewidth]{images/Policy_Iteration.png}}
\end{tabular}

Monotonic improvement of PI:
\begin{itemize}
    \item \scalebox{0.85}{$v^{\pi_{t+1}}(x) \ge v^{\pi_{t}}(x)$} for all $x \in X$
    \item \scalebox{0.85}{$v^{\pi_{t+1}}(x) >v^{\pi_{t}}(x)$} for at least one $x \in X$, unless \scalebox{0.85}{$v^{\pi_t} \equiv v^\star$}
\end{itemize}

\includegraphics[width=0.8\columnwidth]{images/Value_Iteration.png}

\begin{itemize}
    \item VI converges to an optimal policy, as $\fnv[\star]$ and $\fnq[\star]$ are a fixed-points of the Bellman update $\mBs$.
    \item For any $\epsilon > 0$, VI converges to an $\epsilon$-optimal solution in poly time. However, unlike PI, VI does not generally reach the exact optimum in a finite num. of iter.. 
\end{itemize}

\begin{framed}
    A \textbf{Partially observable MDP (POMDP)} is a Markov process, with a set of \textbf{observations} $\sY$, and \textbf{observation probs.} $o(y \mid x) \defeq \Pr{Y_t = y \mid X_t = x}$.
\end{framed}

\begin{itemize}
    \item POMDP are hard to solve in general, but can be reduced to MDP with an enlarged state space.
    \item We consider MDP whose state are \textbf{beliefs}: $b_t(x) \defeq \Pr{X_t = x \mid y_{1:t}, a_{1:t-1}}$. Keeping track of how beliefs change over time is \textbf{Bayesian filtering}: Given a prior belief $b_t$, an action taken $a_t$, and a new observation $y_{t+1}$, the belief state can be updated as: \scalebox{0.7}{$b_{t+1}(x) = \Pr{X_{t+1} = x \mid y_{1:t+1}, a_{1:t}} = \frac{1}{Z} o(y_{t+1} \mid x) \sum_{x' \in \sX} p(x \mid x', a_t) b_t(x')$}, where \scalebox{0.8}{$Z \defeq \sum_{x \in \sX} o(y_{t+1} \mid x) \sum_{x' \in \sX} p(x \mid x', a_t) b_t(x')$}.
    \item The seq. of belief-states defines the seq. of RVs $(B_t)_{t\in\Nat_0}$:$ B_t \defeq X_t \mid y_{1:t}, a_{1:t-1}$, where the (state-)space of all beliefs is the (infinite) space of all prob. distr. over $\sX$: \scalebox{0.75}{$\spB \defeq \Delta^{\sX} \defeq \braces*{\vb \in \R^{\card{\sX}} : \vb \geq \vzero, \textstyle\sum_{i=1}^{\card{\sX}} \vb(i) = 1}$}.
\end{itemize}

\begin{framed}
    Given a POMDP, the \textbf{belief-state MDP} specified by the \textbf{belief space} $\spB \defeq \Delta^{\sX}$ depending on the \textbf{hidden states} $\sX$; the set of \textbf{actions} $\sA$; \textbf{transition probabilities} $\tau(b' \mid b, a) \defeq \Pr{B_{t+1} = b' \mid B_t = b, A_t = a}$; and \textbf{rewards} $\rho(b, a) \defeq \E[x \sim b]{r(x, a)} = \sum_{x \in \sX} b(x) r(x, a)$.
\end{framed}

\scalebox{0.69}{$\tau(b_{t+1} \mid b_t, a_t) = \Pr{b_{t+1} \mid b_t, a_t} = \sum_{y_{t+1} \in \sY} \Pr{b_{t+1} \mid b_t, a_t, y_{t+1}} \Pr{y_{t+1} \mid b_t, a_t}$}\\
We also set: $\Pr{b_{t+1} \mid b_t, a_t, y_{t+1}} = 1$ iff $b_{t+1}$ matches the belief update given $b_t, a_t$, and $y_{t+1}$, and 0 else. Finally the likelihood is: \scalebox{0.8}{$\begin{aligned}[t]\Pr{y_{t+1} \mid b_t, a_t} &= \E[x \sim b_t]{\E[x' \mid x, a_t]{\Pr{y_{t+1} \mid X_{t+1} = x'}}} \\[-1.5ex]
&= \sum_{x \in \sX} b_t(x) \sum_{x' \in \sX} p(x' \mid x, a_t) \cdot o(y_{t+1} \mid x')\end{aligned}$}
