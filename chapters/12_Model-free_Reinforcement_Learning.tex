\vspace{0.2mm}\hrule\vspace{0.2mm}
\section{Model-free Reinforcement Learning}

\vspace{-2pt}
\begin{itemize}
    \item Time req. to compute value fn. for every state-action pair exactly grows poly. in size of state-action space.\\
    \item Can view TD-/Q-learning as SGD on the squared loss:\\[-0.5ex]
    $\ell(\vtheta; x, r, x') \defeq \frac{1}{2}\parentheses{r + \gamma \old{\vtheta}(x') - \vtheta(x)}^2$ 
    and learn param. approx. of $\V{\vx; \vtheta}$ or $\Q{\vx}{\va; \vtheta}$ using Monte Carlo est. and bootstrapping. 
\end{itemize}

\columnbreak

Model-free (TD-/Q-learn.) are usually \textbf{sample ineff.}:
\begin{itemize}
    \item Bootstrapping leads to “(initially) incorrect” and “unstable” targets of the optimization
    \item Monte Carlo est. with single sample leads to large var.
\end{itemize}

\begin{framed}
    \textbf{Q-learning with fn. approx.}: (1) Observe $\vx'$ and $r$ from picking $a$ in $\vx$. (2) Update $\vtheta \gets \vtheta + \alpha_t \delta_\mathrm{B} \vphi(\vx, \va)$, where \scalebox{0.9}{$\delta_\mathrm{B} \defeq r + \gamma \max_{\vap \in \spA} \Q*{\vxp}{\vap; \old{\vtheta}} - \Q*{\vx}{\va; \vtheta}$}.
\end{framed}

\begin{itemize}
    \item In the tabular setting, this is identical to Q-learning 
    \item Converges to the true Q-function $q^\star$.
\end{itemize}

\vspace{0.2mm}\hrule\vspace{0.2mm}

"Tricks of the trade” to improve SGD:
\begin{itemize}
    \item \textbf{Stabilizing opti. targets:} Bootstrapping est. changes after each iteration, leading to stability issues. \textbf{DQN} updates NN used for bootstrapping infrequently and maintains const. opti. target across multiple episodes. E.g. clone: hanging/online NN and fixed/target NN.
    
    \item \textbf{Max. bias:} Estimates $Q^\star$ are noisy (biased) estimates of $q^\star$. \textbf{DDQN}: instead of picking optimal action w.r.t. old network, it picks w.r.t. new network:
\end{itemize}

\begin{framed}
    \textbf{Policy val. fn.:} measures discounted payoff of policy:\\
    $\J{\pi} \defeq \E[\pi]{G_0} = \E[\pi]{\sum_{t=0}^\infty \gamma^t R_t}$, and bounded ver.:\\
    $\J{\pi}[T] \defeq \E[\pi]{G_{0:T}} = \E[\pi]{\sum_{t=0}^{T-1} \gamma^t R_t}$.
    % Abbreviate $\j{\vvarphi} \defeq \j{\pi_\vvarphi}$
    \hfill Non-convex.
\end{framed}

\begin{framed}
    \textbf{Score grad. est.:} \scalebox{0.84}{$\nabla_\vvarphi \E[\tau \sim \Pi_\vvarphi]{G_{0:T}} = \E{G_{0:T} \nabla_\vvarphi \log \Pi_\vvarphi(\tau)}$}\\[-0.5ex]
    Typically var. of est. is very large. Reduce w. \textbf{baselines}:\\
    $\E{G_{0:T} \nabla_\vvarphi \log \Pi_\vvarphi(\tau)} = \E{(G_{0:T} - b) \nabla_\vvarphi \log \Pi_\vvarphi(\tau)}$
\end{framed}

\vspace{0.2mm}\hrule\vspace{0.2mm}
\includegraphics[width=0.8\linewidth,trim={0 1.5cm 0 0.8cm},clip]{images/REINFORCE.png}
\begin{tikzpicture}[baseline, overlay]
  \node[anchor=west, font=\fontsize{6pt}{6pt}\selectfont, anchor=base, xshift=-0.6cm, yshift=1.5cm, fill=yellow!10] at (0, 0) {
    On-policy, Model-free
  };
\end{tikzpicture}

\begin{itemize}
    \item SGD with score grad est. and downstream returns.
    \item Not guaranteed to find an optimal policy. Can get stuck in local optima even for very small domains.
\end{itemize}

\begin{framed}
    \textbf{Advantage fn.}: $\begin{aligned}[t]
        &\a[\pi]{\vx}{\va} \defeq \q[\pi]{\vx}{\va} - \v[\pi]{\vx}\\[-1ex]
        &= \q[\pi]{\vx}{\va} - \E[\vap \sim \pi(\vx)]{\q[\pi]{\vx}{\vap}}\end{aligned}$
    \begin{itemize}
        \item $\text{$\pi$ is optimal} \iff \forall \vx \in \spX, \va \in \spA : \a[\pi]{\vx}{\va} \leq 0$
    \end{itemize}
\end{framed}

\begin{framed}
    \textbf{Policy gradient theorem:} Max. $J(\vvarphi)$ corresponds to incr. the prob. of actions with large and decr. the prob. of actions with small value, taking into account how often the resulting policy visits certain states. \\
    $\nabla_\vvarphi \J{\vvarphi} = \sum_{t=0}^\infty \E[\vx_t, \va_t]{\gamma^t q^{\pi_\vvarphi}(\vx_t, \va_t) \nabla_\vvarphi \pi_\vvarphi (\va_t, \vx_t)}$\\
\end{framed}

% \begin{framed}
%   \textbf{AC} consist of: (1) \textbf{actor}: param. $\pi(\va \mid \vx; \vvarphi) \eqdef \pi_\vvarphi$; and (2) \textbf{critic}: approx. $\q[\pi_\vvarphi]{\vx}{\va} \approx \Q[\pi_\vvarphi]{\vx}{\va; \vtheta}$.
% \end{framed}

% Use gradient approximation: \tiny{$\grad_\vvarphi \J{\vvarphi} \approx \sum_{t=0}^\infty\E[(\vx_t,\va_t) \sim \pi_\vvarphi]{\gamma^t \Q{\vx_t}{\va_t; \vtheta} \grad_\vvarphi \log \pi_\vvarphi(\va_t \mid \vx_t)}$}

\vspace{0.2mm}\hrule\vspace{0.2mm}
\includegraphics[width=0.7\linewidth,trim={0 0.9cm 1.5cm 0.6cm},clip]{images/Online_actor_critic.png}
\begin{tikzpicture}[baseline, overlay]
  \node[anchor=west, font=\fontsize{6pt}{6pt}\selectfont, anchor=base, xshift=-0.4cm, yshift=2cm, fill=yellow!10] at (0, 0) {
    On-policy, Online, Model-free
  };
\end{tikzpicture}

\vspace{-1.5cm}
\begin{list}{}{
    \leftmargin=3cm
    \rightmargin=0cm
    \itemsep=1pt % Space between different items
    \parsep=1pt  % Space between paragraphs/wrapped lines within an item
    \topsep=0pt   % Space above the list
    \partopsep=0pt
}
    \item \scalebox{0.7}{\textbullet} Use SARSA for learning critic; SGD for gradient est.
    \item \scalebox{0.7}{\textbullet} Actor is not guaranteed to improve
\end{list}
\vspace{1pt}
\begin{list}{}{
    \leftmargin=2.2cm
    \rightmargin=0cm
    \itemsep=1pt % Space between different items
    \parsep=1pt  % Space between paragraphs/wrapped lines within an item
    \topsep=0pt   % Space above the list
    \partopsep=0pt
}
    \item \scalebox{0.7}{\textbullet} Typically rely on randomization in policy to encourage exploration
\end{list}
\vspace{-1pt}
\begin{list}{}{
    \leftmargin=0cm
    \rightmargin=0cm
    \itemsep=1pt % Space between different items
    \parsep=1pt  % Space between paragraphs/wrapped lines within an item
    \topsep=0pt   % Space above the list
    \partopsep=0pt
}
    \item \scalebox{0.7}{\textbullet} Typically low sample efficiency (improve. w. TRPO)
\end{list}

Reduce var. of the grad estimates version with \textbf{A2C}:\vspace{-2pt}
\includegraphics[width=0.7\linewidth,trim={0.5cm 0.3cm 0cm 0.3cm},clip]{images/A2C.png}
\hspace{-0.7cm}w. Bootstrap est. $\va^{\pi_\vvarphi}$

\hrule\vspace{0.2mm}

{\setlength{\baselineskip}{6.8pt}
 \setlength{\lineskiplimit}{-12pt}
 \hyphenpenalty=0 \exhyphenpenalty=0
    \scalebox{0.7}{\textbullet}~\textbf{TRPO:} KL-constraint forces monotonic improvement via trust regions \hl{(on-pol., m-free, pol.-gradient)}
    \scalebox{0.7}{\textbullet}~\textbf{PPO:} Clipped obj. stabilizes training by limiting update size \hl{(on-pol., m-free, AC)}
    \scalebox{0.7}{\textbullet}~\textbf{GRPO:} Group-relative advantages remove need for separate critic \hl{(on-pol., m-free, critic-less)}
    \scalebox{0.7}{\textbullet} \textbf{DDPG:} Det. grads. enable cont. control with target networks, inject Gaussian noise “dithering” \hl{(off-pol., m-free, AC)}
    \scalebox{0.7}{\textbullet}~\textbf{SAC:} Max. entropy obj. balances exploration and performance \hl{(off-pol., m-free)}
    \scalebox{0.7}{\textbullet}~\textbf{DPO:} No explicit reward modeling \hl{(offline, m-free, ref.-based)}
    \scalebox{0.7}{\textbullet}~\textbf{PETS:} Ensembles plus MPC handle uc. in planning. \hl{(m-based)}
    \scalebox{0.7}{\textbullet}~\textbf{UCRL:} selects optimistic MDPs for efficient exploration \hl{(m-based, exploration, o.i.t.f.o.uc)}
    \scalebox{0.7}{\textbullet}~\textbf{H-UCRL:} hallucinated transitions ensure safe, robust exploration. \hl{(m-based, opt. o.i.t.f.o.uc, hierarchical)}
\par}
